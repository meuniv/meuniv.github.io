<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>3. Mathematical Interlude: Very Large Numbers &mdash; Introduction to Statistical Mechanics</title>
    
    <link rel="stylesheet" href="_static/pyramid.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Introduction to Statistical Mechanics" href="index.html" />
    <link rel="next" title="4. The Interpretation of Statistical Quantities" href="interpretation.html" />
    <link rel="prev" title="2. The Statistical Description of Physical Systems" href="statisticaldescription.html" />
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Neuton&amp;subset=latin" type="text/css" media="screen" charset="utf-8" />
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Nobile:regular,italic,bold,bolditalic&amp;subset=latin" type="text/css" media="screen" charset="utf-8" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->

  </head>
  <body role="document">

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="interpretation.html" title="4. The Interpretation of Statistical Quantities"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="statisticaldescription.html" title="2. The Statistical Description of Physical Systems"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Introduction to Statistical Mechanics</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="mathematical-interlude-very-large-numbers">
<span id="very-large-numbers"></span><h1>3. Mathematical Interlude: Very Large Numbers<a class="headerlink" href="#mathematical-interlude-very-large-numbers" title="Permalink to this headline">¶</a></h1>
<p>Problems in statistical mechanics tend to involve large numbers.  Very <em>very</em> large numbers.  For example, we often
need to average over all the atoms or molecules in a system.  For any macroscopic system, that will be on the order of
Avogadro&#8217;s number: 10<sup>23</sup>, plus or minus a few orders of magnitude.  In other cases we average over microstates,
the number of which usually grows <em>exponentially</em> in the number of atoms.  These are really astronomically large
numbers.</p>
<p>This has an interesting result.  In many cases, we can simply treat averages as exact numbers.  Any variation about the
average value is so small, we can completely ignore it.  This is a great simplification, and is one of the reasons
statistical mechanics is so successful.</p>
<p>It also is the reason that, despite its name, statistical mechanics involves very little real statistics.  A large part
of the field of statistics deals with the variations about averages: computing the probability of a distribution
producing values in a certain range, determining whether a measured value might plausibly have come from a certain
distribution, etc.  But in statistical mechanics, we can usually ignore these questions.  The chance of measuring any
value <em>except</em> the average one is negligible.</p>
<p>The main goal of this chapter is to demonstrate this fact.  We will do that first through a simple example, and then
through a very general theorem.</p>
<div class="section" id="the-binomial-distribution">
<span id="id1"></span><h2>3.1. The Binomial Distribution<a class="headerlink" href="#the-binomial-distribution" title="Permalink to this headline">¶</a></h2>
<p>Do you ever worry that all the oxygen molecules in the air around you might spontaneously migrate to the other side of
the room, leaving you unable to breathe?  It could happen.  Each molecule diffuses independently, so there is a small
but finite probability that just by chance, they will all end up in the same half of the room at the same time.</p>
<p>I hope I am not worrying you.  While this is theoretically possible, it is so absurdly improbable that you really can
ignore it.  We will first compute just how improbable it is, and then study the statistics of this problem in more
detail.</p>
<p>Let <span class="math">\(N\)</span> be the number of oxygen molecules in the room.  We will describe the position of each one simply by
whether it is in the left or right half of the room.  That gives <span class="math">\(2^N\)</span> possible arrangements of molecules, every
one of which is equally probable.  Of all those arrangements, only one has all the oxygen in the opposite half of the
room from you, so the probability is <span class="math">\(1/2^N\)</span>.</p>
<p>Depending on the size of the room you are in, it probably has somewhere in the vicinity of 10<sup>26</sup> oxygen
molecules in it.  So the probability of all the oxygen being on the same side of the room at any given moment is</p>
<div class="math">
\[\frac{1}{2^{10^{26}}} \approx \frac{1}{10^{10000000000000000000000000}}\]</div>
<p>That is a very, very large denominator.  For comparison here are some much smaller numbers:</p>
<ul class="simple">
<li>The age of the universe is on the order of 10<sup>17</sup> seconds.</li>
<li>There are somewhere around 10<sup>80</sup> atoms in the visible universe.</li>
</ul>
<p>In short, you really do not need to worry about this happening.  You would be much better off worrying about more likely
events.  For example, that you will be struck by lightning and then, as you try to pick yourself back up, hit by
a meteor falling from space.  And then immediately be struck by lightning a second time just for good measure.  That is
the sort of thing that is far more likely to happen to you.</p>
<p>Perhaps you are still not reassured.  You may have realized that even if only 90% of the oxygen moved to the other side
of the room, that would still leave you in a rather awkward position.  We really want to work out the entire probability
distribution.  Let <span class="math">\(m\)</span> be the number of oxygen molecules in the left half of the room and <span class="math">\(N-m\)</span> the number
in the right half.  Assuming the energy of each molecule is independent of which side of the room it is on, and that the
oxygen molecules are far enough apart that we can ignore their interactions (both reasonable assumptions), the
probability is simply proportional to <span class="math">\(\Omega(m)\)</span>.  Let us consider how that varies with <span class="math">\(m\)</span>.</p>
<ul class="simple">
<li>There is precisely one arrangement of molecules that has all of them on the left side of the room, so
<span class="math">\(\Omega(0)=1\)</span>.</li>
<li>There are <span class="math">\(N\)</span> possible ways to have exactly one molecule on the left side of the room (any of the <span class="math">\(N\)</span>
molecules could be the one), so <span class="math">\(\Omega(1)=N\)</span>.</li>
<li>To get two molecules on the left side of the room, we have <span class="math">\(N\)</span> choices for the first molecule and <span class="math">\(N-1\)</span>
for the second molecule.  But now we have counted each state twice: for any pair of molecules, it does not matter which
order we choose them in.  So <span class="math">\(\Omega(2)=N(N-1)/2\)</span>.</li>
</ul>
<p>Continuing like this, we find the general form:</p>
<div class="math" id="equation-binomialomega">
<span class="eqno">(1)</span>\[\Omega(m) = \frac{N!}{m!(N-m)!}\]</div>
<p>The probability is then</p>
<div class="math" id="equation-binomialprobability">
<span class="eqno">(2)</span>\[p(m) = \frac{1}{2^N} \frac{N!}{m!(N-m)!}\]</div>
<p>This is a special case of the <em>binomial distribution</em>.  (In the general case, the probability for a particle to be on
the left side of the room could be different from its probability to be on the right side, but for this example we can
ignore that.)  It is shown in Figure 3-1 for several values of <span class="math">\(N\)</span>.  Notice how, as <span class="math">\(N\)</span>
increases, the probability becomes steadily more concentrated around its midpoint.</p>
<div class="figure align-center" id="id2">
<a class="reference internal image-reference" href="_images/binomial.svg"><img src="_images/binomial.svg" /></a>
<p class="caption"><span class="caption-text">Figure 3-1. The binomial distribution for several values of <span class="math">\(N\)</span>.  To make them easier to
compare, all the curves have been normalized to go from 0 to 1 along each axis.</span></p>
</div>
<p>The name comes from the fact that it has the same form as the terms in the <em>binomial series</em>:</p>
<div class="math" id="equation-binomialseries">
<span class="eqno">(3)</span>\[(a+b)^N = \sum_{m=0}^N \frac{N!}{m!(N-m)!}a^Nb^{N-m}\]</div>
<p>If we let <span class="math">\(a=b=1/2\)</span>, this simplifies to</p>
<div class="math">
\[\left(\frac{1}{2}+\frac{1}{2}\right)^N = \sum_{m=0}^N \frac{N!}{m!(N-m)!} \frac{1}{2^N}\]</div>
<p>so that</p>
<div class="math" id="equation-binomialsum">
<span class="eqno">(4)</span>\[\sum_{m=0}^N \frac{N!}{m!(N-m)!} = 2^N\]</div>
<p>Let us examine this probability distribution a bit further.  We can roughly characterize it by calculating its mean and
standard deviation.  The mean is easy: every particle has an equal chance to be on each side of the room, so
<span class="math">\(\langle m \rangle = N/2\)</span>.  Another way to think about this is that the distribution is symmetric: for any value
of <span class="math">\(m\)</span>, <span class="math">\(p(m)=p(N-m)\)</span>, from which it directly follows that the mean must be <span class="math">\(N/2\)</span>.</p>
<p>The standard deviation is a bit more complicated to derive.  As a first step, let us first calculate</p>
<div class="math">
\[\langle m(m-1) \rangle = \frac{1}{2^N} \sum_{m=0}^N m(m-1) \frac{N!}{m!(N-m)!}\]</div>
<p>Notice that the first two terms of the sum are both zero, so we can increase the lower bound to begin from 2.  This
allows us to then cancel out the factors of <span class="math">\(m(m-1)\)</span> that appear in both numerator and denominator:</p>
<div class="math">
\[\begin{split}\langle m(m-1) \rangle &amp;= \frac{1}{2^N} \sum_{m=2}^N \frac{N!}{(m-2)!(N-m)!} \\
&amp;= \frac{N(N-1)}{2^N} \sum_{m=2}^N \frac{(N-2)!}{(m-2)!(N-m)!}\end{split}\]</div>
<p>Now make two substitutions: define <span class="math">\(X=N-2\)</span> and <span class="math">\(y=m-2\)</span>.  This simplifies it to:</p>
<div class="math">
\[\langle m(m-1) \rangle = \frac{N(N-1)}{2^N} \sum_{y=0}^X \frac{X!}{y!(X-y)!}\]</div>
<p>We immediately recognize the sum as being the same one that appeared in equation <a href="#equation-binomialsum">(4)</a>.  Replacing it by
<span class="math">\(2^X=2^{N-2}\)</span>,</p>
<div class="math">
\[\langle m(m-1) \rangle = \frac{N(N-1)2^{N-2}}{2^N} = \frac{N(N-1)}{4}\]</div>
<p>We now have all the pieces we need.  Recall that</p>
<div class="math" id="equation-binomialvariance">
<span class="eqno">(5)</span>\[\begin{split}Var(m) &amp;= \langle m^2 \rangle - \langle m \rangle^2 \\
&amp;= \langle m(m-1) \rangle + \langle m \rangle- \langle m \rangle^2 \\
&amp;= \frac{N(N-1)}{4} + \frac{N}{2} - \frac{N^2}{4} \\
&amp;= \frac{N}{4}\end{split}\]</div>
<p>The standard deviation is then</p>
<div class="math" id="equation-binomialstddev">
<span class="eqno">(6)</span>\[\sigma \equiv \sqrt{Var(m)} = \frac{\sqrt{N}}{2}\]</div>
<p>Let us take a moment to consider these results.  The average value is proportional to <span class="math">\(N\)</span>, while the standard
deviation is proportional to <span class="math">\(\sqrt{N}\)</span>.  In most cases, what we really care about is the ratio of the two.  You
want to know what <em>fraction</em> of the oxygen is likely to be on one side of the room, not the specific number of
molecules.  A number that would be huge in a broom closet would be negligible in an auditorium.  Taking the ratio gives</p>
<div class="math" id="equation-binomialfractionaldeviation">
<span class="eqno">(7)</span>\[\frac{\sigma}{\langle m \rangle} = \frac{1}{\sqrt{N}}\]</div>
<p>For an average sized room with 10<sup>26</sup> oxygen molecules, that equals 10<sup>-13</sup>.  This is the
magnitude of the typical fluctuations, measured as a fraction of the total number of molecules.  There are very few
physical quantities that can actually be measured to a precision of 13 significant digits, and this is not one of them.
The random fluctuations in the number of molecules on each side of the room are simply too small to measure.</p>
</div>
<div class="section" id="the-central-limit-theorem">
<h2>3.2. The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Permalink to this headline">¶</a></h2>
<p>Having worked through one example in detail, you might wonder how general our conclusions are.  Do other types of
statistical problems behave roughly the same way?  The answer is yes.  There is a very general theorem which guarantees
that a wide range of quantities will scale in essentially the same way.</p>
<p>Before presenting that theorem, I first need to introduce one other very important probability distribution, known as
the <em>Gaussian</em> or <em>normal</em> distribution:</p>
<div class="math" id="equation-gaussianprobability">
<span class="eqno">(8)</span>\[p(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{\frac{-(x-\mu)^2}{2 \sigma^2}}\]</div>
<p>It is shown in Figure 3-2.  It is not really as complicated as it looks.  The basic form is just <span class="math">\(e^{-x^2}\)</span>.
Replacing <span class="math">\(x\)</span> by <span class="math">\(x-\mu\)</span> shifts the mean of the distribution to <span class="math">\(\mu\)</span>, and then the exponent is scaled
to make the standard deviation equal <span class="math">\(\sigma\)</span>.  The factor in front is just a normalization, to ensure that</p>
<div class="math">
\[\int_{-\infty}^{\infty} p(x) dx = 1\]</div>
<div class="figure align-center" id="id3">
<a class="reference internal image-reference" href="_images/gaussian.svg"><img src="_images/gaussian.svg" /></a>
<p class="caption"><span class="caption-text">Figure 3-2. A normal distribution with mean of 0 and standard deviation of 1.</span></p>
</div>
<p>The name &#8220;normal distribution&#8221; gives a sense of just how important this distribution is in statistics.  It comes up
constantly.  We will see the reason for this in just a moment.</p>
<p>In the last section, we considered the number of oxygen molecules <span class="math">\(m\)</span> in the left half of the room.  You can think
of this quantity as a sum of random values:</p>
<div class="math">
\[m = \sum_{i=1}^{N} x_i\]</div>
<p>Each value <span class="math">\(x_i\)</span> is either 0 or 1 (depending on which half of the room the molecule is in) with equal probability.
Now we will generalize this to let the values be drawn from an arbitrary distribution.  That distribution could be
either discrete or continuous.  The only thing we will assume about it is that it has a known mean and standard
deviation.  There is a remarkable result called the <em>Central Limit Theorem</em>:</p>
<div class="admonition-the-central-limit-theorem admonition">
<p class="first admonition-title">The Central Limit Theorem</p>
<p>Consider the sum</p>
<div class="math">
\[S = \sum_{i=1}^N x_i\]</div>
<p class="last">where the values <span class="math">\(x_i\)</span> are independently drawn from a distribution with mean <span class="math">\(\mu_x\)</span> and standard
deviation <span class="math">\(\sigma_x\)</span>.  In the limit <span class="math">\(N \to \infty\)</span>, the sum <span class="math">\(S\)</span> is distributed according to a
normal distribution with mean <span class="math">\(\mu = N \mu_x\)</span> and standard deviation <span class="math">\(\sigma = \sqrt{N} \sigma_x\)</span>.</p>
</div>
<p>This theorem dates back to 1718, when Abraham de Moivre presented a proof of a special case of it in his book
<em>The Doctrine of Chances</em>.  This book was not, as you might guess, an academic treatise for mathematicians.  It was a
book for gamblers, discussing how to win at various games of chance.  Don&#8217;t ever let anyone tell you that mathematics
is not useful!</p>
<p>This is the reason that normal distributions are so important.  No matter what distribution you start out with, once
you add enough values together the result will always be normally distributed.  How large <span class="math">\(N\)</span> needs to be varies,
of course, depending on the initial distribution, but it often does not need to be very large.  In many cases, summing
over 10 values is already enough to give quite a good approximation to a normal distribution.</p>
<p>Notice that our main conclusions from the previous section apply just as well to the general case as they did for our
specific example.  The standard deviation is proportional to <span class="math">\(\sqrt{N}\)</span>, the mean is proportional to <span class="math">\(N\)</span>,
and their ratio scales as <span class="math">\(1/\sqrt{N}\)</span>.  No matter what distribution the individual values come from, once you add
up a macroscopic number of them the fluctuations will be negligibly small.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">3. Mathematical Interlude: Very Large Numbers</a><ul>
<li><a class="reference internal" href="#the-binomial-distribution">3.1. The Binomial Distribution</a></li>
<li><a class="reference internal" href="#the-central-limit-theorem">3.2. The Central Limit Theorem</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="statisticaldescription.html"
                        title="previous chapter">2. The Statistical Description of Physical Systems</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="interpretation.html"
                        title="next chapter">4. The Interpretation of Statistical Quantities</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="interpretation.html" title="4. The Interpretation of Statistical Quantities"
             >next</a> |</li>
        <li class="right" >
          <a href="statisticaldescription.html" title="2. The Statistical Description of Physical Systems"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Introduction to Statistical Mechanics</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2014-2015, Peter Eastman.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>