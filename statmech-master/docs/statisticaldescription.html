<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>2. The Statistical Description of Physical Systems &mdash; Introduction to Statistical Mechanics</title>
    
    <link rel="stylesheet" href="_static/pyramid.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Introduction to Statistical Mechanics" href="index.html" />
    <link rel="next" title="3. Mathematical Interlude: Very Large Numbers" href="largenumbers.html" />
    <link rel="prev" title="1. Introduction" href="introduction.html" />
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Neuton&amp;subset=latin" type="text/css" media="screen" charset="utf-8" />
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Nobile:regular,italic,bold,bolditalic&amp;subset=latin" type="text/css" media="screen" charset="utf-8" />
<!--[if lte IE 6]>
<link rel="stylesheet" href="_static/ie6.css" type="text/css" media="screen" charset="utf-8" />
<![endif]-->

  </head>
  <body role="document">

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="largenumbers.html" title="3. Mathematical Interlude: Very Large Numbers"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="introduction.html" title="1. Introduction"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Introduction to Statistical Mechanics</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="the-statistical-description-of-physical-systems">
<span id="statistical-description-of-physical-systems"></span><h1>2. The Statistical Description of Physical Systems<a class="headerlink" href="#the-statistical-description-of-physical-systems" title="Permalink to this headline">¶</a></h1>
<p>Given a physical system, how do you apply statistics to describe it?  What does that even mean?  There are lots of
aspects of the system you might try to compute statistics for, but what approach is the most useful?  This chapter will
attempt to answer these questions by introducing the basic techniques and concepts of statistical mechanics.</p>
<p>Along the way, we will define quantities with names like &#8220;temperature&#8221;, &#8220;pressure&#8221;, and &#8220;entropy&#8221;.  Of course you have
heard these words before and you probably have an idea of what you think they mean.  For the moment, try to put those
ideas aside.  In Chapter <a class="reference internal" href="interpretation.html#interpretation-of-statistical-quantities">4</a> we will come back to them and try to build up
an intuitive understanding of these quantities.  But for now, just treat them as mathematical definitions.</p>
<div class="section" id="microstates-and-macrostates">
<span id="id1"></span><h2>2.1. Microstates and Macrostates<a class="headerlink" href="#microstates-and-macrostates" title="Permalink to this headline">¶</a></h2>
<p>To describe an isolated physical system with statistics, we begin by making the following fundamental assumption:</p>
<div class="admonition-assumption admonition">
<p class="first admonition-title">Assumption</p>
<p class="last">The system can exist in a discrete (but possibly infinite) set of <em>microstates</em>.  A microstate defines the values
of all possible <em>microscopic variables</em>.</p>
</div>
<p>In a classical system of point particles, for example, a microstate defines the position and momentum of every particle.
In a quantum mechanical system, it defines the value of the wavefunction at every point in space.  To specify what
microstate the system is in, you must give the most detailed description you will ever care about.</p>
<p>You may be wondering whether this is a valid assumption.  What if the microstates are not discrete?  For example, the
position and momentum of a classical particle are continuous, not discrete.  Whether the universe is actually discrete
or continuous at its most fundamental level is still an open question.</p>
<p>Fortunately, this turns out not to matter very much.  You can always turn a continuous variable into a discrete one by
dividing it into very small bins.  For the classical particle, we treat all positions between <span class="math">\(x\)</span> and
<span class="math">\(x+\delta x\)</span> and all momenta between <span class="math">\(p\)</span> and <span class="math">\(p+\delta p\)</span> as a single microstate.  As long as we
choose <span class="math">\(\delta x\)</span> and <span class="math">\(\delta p\)</span> sufficiently small, the exact values turn out to have no effect on most
of our results.</p>
<p>This is illustrated in Figure 2-1, which shows the space of possible microstates for a single particle in one dimension.
Each microstate is defined by its values of <span class="math">\(x\)</span> and <span class="math">\(p\)</span>.  This space is known as <em>phase space</em>, and we will
use it often.  More generally, a system of <span class="math">\(N\)</span> particles in <span class="math">\(d\)</span> dimensions has a <span class="math">\(2dN\)</span> dimensional
phase space.  Every point (or rather, each tiny volume as shown in Figure 2-1) in this phase space represents a
microstate.</p>
<div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="_images/phase_space.svg"><img src="_images/phase_space.svg" /></a>
<p class="caption"><span class="caption-text">Figure 2-1. The phase space of a single particle in one dimension.  Each axis is divided into
tiny intervals.  The volume defined by the intersection of one interval from each axis is a microstate.</span></p>
</div>
<p>For typical systems, the number of microstates is huge and they describe the system in far more detail than we usually
care about.  Consider a box filled with gas.  You have no way to measure the exact position and momentum of every last
gas molecule, and you would not care about them even if you could measure them.  Instead, you are usually interested in
a small number of <em>macroscopic variables</em>: the total energy of the system, the total number of gas molecules, the volume
of space it takes up, etc.  These are things you can measure and that have practical importance.</p>
<p>A <em>macrostate</em> is defined by specifying the value of every macroscopic variable.  There may be a huge number of
microstates all corresponding to the same macrostate.  For example, suppose you measure the total energy and volume of
a box of gas.  There are an enormous number of arrangements of the individual gas molecules that all add up to that
energy and volume.  You know the gas is in <em>one</em> of those states, but you have no idea <em>which</em> one.</p>
</div>
<div class="section" id="the-density-of-states">
<span id="id2"></span><h2>2.2. The Density of States<a class="headerlink" href="#the-density-of-states" title="Permalink to this headline">¶</a></h2>
<p>The number of microstates corresponding to a macrostate is called the <em>density of states</em>.  It is written
<span class="math">\(\Omega(E, V, \dots)\)</span>, where the arguments are the macroscopic variables defining the macrostate.</p>
<p>Since we assumed the microstates are discrete, that means <span class="math">\(\Omega\)</span> is also a discrete function.  Each
variable can only take on specific values, and it is only defined for those values.  In practice, however, we usually
treat <span class="math">\(\Omega\)</span> as a continuous function.  As long as the allowed values are sufficiently close together, the total
number of states in any given interval will be huge and we can simply think of <span class="math">\(\Omega\)</span> as the number of
microstates per unit volume in the space defined by the macroscopic variables.  That is why we call it the &#8220;density&#8221; of
states.</p>
<div class="admonition-example admonition">
<p class="first admonition-title">Example</p>
<p>Compute <span class="math">\(\Omega(E)\)</span> for a single free particle in three dimensions.</p>
<p>The energy is given by <span class="math">\(E=|\mathbf{p}|^2/2m\)</span>.  The microstates are defined by a six dimensional phase space,
but since the energy does not depend on the positions, we can ignore them and just consider the three dimensional
space of momentum coordinates.</p>
<p>Each value of <span class="math">\(E\)</span> corresponds to a two dimensional surface in this three dimensional space, so <span class="math">\(\Omega\)</span>
is proportional to the area of that surface.  In this case, it is just a sphere of radius <span class="math">\(|\mathbf{p}|\)</span>.
Since the surface area of a sphere is equal to <span class="math">\(4 \pi r^2\)</span>,</p>
<div class="last math">
\[\Omega(E) \propto |\mathbf{p}|^2 \propto E\]</div>
</div>
<p>The proportionality constant in this example depends on what value we choose for <span class="math">\(\delta p\)</span> in discretizing the
momenta.  In most cases, however, we will only be interested in the ratios of <span class="math">\(\Omega\)</span> for different values of
<span class="math">\(E\)</span>, not in its absolute value for single energies, so we can ignore it.</p>
<p>In this example, the density of states increases with increasing energy.  That is nearly always true.  In this case it
grows only linearly with <span class="math">\(E\)</span>, but it often grows much faster.  Consider another example.</p>
<div class="admonition-example admonition">
<p class="first admonition-title">Example</p>
<p>Compute <span class="math">\(\Omega(E)\)</span> for a collection of <span class="math">\(N\)</span> free particles in three dimensions.</p>
<p>This is nearly identical to the previous example, except we now have <span class="math">\(3N\)</span> momentum coordinates.  Each value of
<span class="math">\(E\)</span> corresponds to a <span class="math">\(3N-1\)</span> dimensional surface.  Combining all the momenta into a single vector, we can
write the result as</p>
<div class="last math">
\[\Omega(E) \propto |\mathbf{p}|^{3N-1} \propto E^{(3N-1)/2}\]</div>
</div>
<p>For macroscopic systems, <span class="math">\(N\)</span> will be on the order of 10<sup>23</sup>, and even a tiny increase in energy will
produce a huge increase in the density of states.  For this reason, we usually work with <span class="math">\(\mathrm{log}(\Omega)\)</span> instead,
which is a much smoother, slower changing function.</p>
</div>
<div class="section" id="the-postulate-of-equal-a-priori-probabilities">
<h2>2.3. The Postulate of Equal <em>a priori</em> Probabilities<a class="headerlink" href="#the-postulate-of-equal-a-priori-probabilities" title="Permalink to this headline">¶</a></h2>
<p>Suppose you have measured a set of macroscopic variables for an isolated system.  You now know which macrostate it is
in, but there may be a huge number of microstates all consistent with that macrostate.  What can you say about which one
of those it is most likely to be in?</p>
<p>The answer, of course, is that you have no idea.  Your measurements do not provide any further information to answer
that question.  Nonetheless, to calculate any averages or other statistical quantities you must assume something.  This
leads us to the following assumption:</p>
<div class="admonition-the-postulate-of-equal-a-priori-probabilities admonition">
<p class="first admonition-title">The Postulate of Equal <em>a priori</em> Probabilities</p>
<p class="last">A system has an equal probability of being in any microstate that is consistent with its current macrostate.</p>
</div>
<p>Is there any justification for this assumption?  That turns out to be a very complicated question.  In practice it works
very well for many different cases, and there are theoretical arguments for it.  On the other hand, there certainly
are cases where it is not correct.  So instead we will approach the question from a slightly different direction and
treat it as a definition:</p>
<div class="admonition-definition admonition">
<p class="first admonition-title">Definition</p>
<p class="last">An isolated system that satisfies the postulate of equal <em>a priori</em> probabilities is said to be in <em>equilibrium</em>.</p>
</div>
<p>(But note that this definition is usually written in a slightly different way.  We will come back to it in the next
section.)</p>
<p>For the initial part of this book, we will only consider systems in equilibrium.  We will therefore treat this postulate
as a given.  Of course, there are many interesting situations involving systems that are <em>not</em> in equilibrium.  We will
examine some of those later in the book.  We will also look at the process by which systems come to be in equilibrium,
and what happens if they are then disturbed from it.</p>
</div>
<div class="section" id="time-averages-and-ensemble-averages">
<h2>2.4. Time Averages and Ensemble Averages<a class="headerlink" href="#time-averages-and-ensemble-averages" title="Permalink to this headline">¶</a></h2>
<p>I was a bit careless with terminology in the last section.  I spoke of the &#8220;probability&#8221; of a system being in a
particular microstate, but never defined what that meant.  After all, at any given moment the system <em>is</em> in a
particular microstate and not in any other.  There is no probability about it.</p>
<p>There are two approaches one can take to defining probabilities in this context.  The first is to realize that the
degrees of freedom making up the system are constantly changing.  At one instant the atoms have particular
positions and momenta, but the next instant they are different.  We therefore define the probability of the system
being in a microstate as the <em>fraction of time</em> it spends in that state.  Averages computed using this definition of
probability are called <em>time averages</em>.</p>
<p>This is the older of the two definitions.  Boltzmann used this definition in most of his work.  It turns out to have
problems, however, which led to the introduction of a new definition of probability.</p>
<p>Instead of looking at just one system, imagine preparing many identical systems by following the exact same procedure
many times.  All of these systems are in the same macrostate, but each one is in a different microstate.  You have to
simply imagine doing this, because in practice you have only one system in one microstate—but you do not know which one
it is.  We define the probability of the system being in a microstate as the <em>fraction</em> of these imaginary systems that
are in that state. The set of all the systems is called a <em>statistical ensemble</em>, and averages computed with this
definition of probability are called <em>ensemble averages</em>.</p>
<p>You might wonder whether these definitions are equivalent.  Do both types of averages give the same results?  The answer
is an emphatic, &#8220;Sometimes.&#8221;  For some systems they do, and for other system they do not.  We therefore turn this into
another definition:</p>
<div class="admonition-definition admonition">
<p class="first admonition-title">Definition</p>
<p class="last">A system for which time averages and ensemble averages are equal is said to be <em>ergodic</em>.</p>
</div>
<p>(Isn&#8217;t that a wonderful word?  Ergodic.  It comes from the Greek words for &#8220;work&#8221; and &#8220;path&#8221;.  I recommend working it
into your conversation frequently.)</p>
<p>Having hopefully clarified that, we should reconsider the definition from the previous section.  I said that a system
satisfying the postulate of equal <em>a priori</em> probabilities (can I abbreviate that PoEapP?) is in equilibrium.  That is
not the definition you will find in most books.  Here is the more common definition:</p>
<div class="admonition-definition-take-2 admonition">
<p class="first admonition-title">Definition (take 2)</p>
<p class="last">An isolated system is in <em>equilibrium</em> if the probability distribution of its microstates does not change with time.</p>
</div>
<p>It can be proven that if an isolated system ever satisfies the PoEapP, even for a moment, it will then continue to
satisfy it forever after.  So what I said before was certainly true: an isolated system that satisfies the PoEapP is in
equilibrium by either definition.  Furthermore, when using ensemble averages, the PoEapP is usually true by definition.
To perform any calculation you must first specify what statistical ensemble you are using, and ensembles nearly always
stipulate that macroscopically indistinguishable microstates have equal probabilities.  (Whether an ensemble is a good
description of a particular physical system is a different question, of course, one which must be answered by
experiment.)</p>
<p>But if you use time averages, the situation is more complicated.  A system is initially in some particular microstate.
It will then proceed through a series of other microstates as time passes, but there is no reason it <em>must</em> pass through
every microstate that is macroscopically indistinguishable from the original one.  If the system is not ergodic, it
might only pass through a subset of them, never going into others.  Its probability distribution would not satisfy the
PoEapP, but it would still be constant with time.</p>
<p>In this book we will usually work with ensemble averages.  Unless I specifically say otherwise, you should always
assume that probabilities are defined by a statistical ensemble, not by an average over time.</p>
</div>
<div class="section" id="the-maxwell-boltzmann-distribution">
<h2>2.5. The Maxwell-Boltzmann Distribution<a class="headerlink" href="#the-maxwell-boltzmann-distribution" title="Permalink to this headline">¶</a></h2>
<p>There is only so much to say about isolated systems.  Real systems are almost never isolated.  They are embedded in some
sort of environment, and the interaction with that environment is responsible for much of their complexity.</p>
<p>We can use a simple trick to extend our analysis to non-isolated systems.  Begin with an isolated system, then split it
into two parts.  Call them A and B.  A is the part we really care about, the thing we want to do experiments on.  B is
the environment it is connected to.  We only care about B to the extent that it affects A.  It is called a <em>heat bath</em>.</p>
<p>Here are some examples of the sort of thing I mean:</p>
<ul class="simple">
<li>Part A is the gas contained in a box.  Part B is the box itself, along with the whole room the box is sitting in.</li>
<li>Part A is a test tube with chemicals in it.  Part B is a water bath the test tube is sitting in.</li>
<li>The whole system is the air in a room.  Part A is the carbon dioxide molecules in the air.  Part B is everything else
(the nitrogen, oxygen, and other trace gasses).</li>
</ul>
<p>The energy of the system can be decomposed as</p>
<div class="math">
\[E_T = E_A + E_B + E_{AB}\]</div>
<p>The total energy of the system is the sum of three terms: one that depends only on the degrees of freedom that make up
A, one that depends only on the degrees of freedom that make up B, and one that depends on both parts of the system.</p>
<p>We now make a series of assumptions.</p>
<div class="admonition-assumption admonition">
<p class="first admonition-title">Assumption</p>
<p><span class="math">\(E_{AB}\)</span> is small enough that we can ignore it and write</p>
<div class="last math" id="equation-assume-interaction-energy-small">
<span class="eqno">(1)</span>\[E_T \approx E_A + E_B\]</div>
</div>
<p>This is a somewhat odd assumption.  If <span class="math">\(E_{AB}\)</span> were really zero, the two subsystems would not interact at all,
and we would just have two independent isolated systems.  Obviously that is not what we want.  But we do want them to be
<em>weakly coupled</em>.  <span class="math">\(E_{AB}\)</span> should be nonzero, but still much smaller than either <span class="math">\(E_A\)</span> or <span class="math">\(E_B\)</span>.
Actually, what we really care about is that it is much smaller than the <em>variations</em> in the energies of A and B.  If the
energy of A increases, we assume that energy has primarily come out of B, not just from a reduction in the interaction
energy between A and B.</p>
<div class="admonition-assumption admonition">
<p class="first admonition-title">Assumption</p>
<p>The degrees of freedom of A and B are specified independently so the density of states factorizes:</p>
<div class="last math" id="equation-assume-omega-factorizes">
<span class="eqno">(2)</span>\[\Omega_T = \Omega_A \Omega_B\]</div>
</div>
<p>This is another aspect of requiring the subsystems to be weakly coupled.  The first assumption restricted them from
being coupled through the energy function.  This one restricts them from being coupled through the definitions of their
degrees of freedom.  We must be free to choose a state for A, and then independently to choose a state for B.  One must
not restrict the other.</p>
<div class="admonition-assumption admonition">
<p class="first admonition-title">Assumption</p>
<p>The log of the density of states of B can be approximated as linear in energy:</p>
<div class="last math" id="equation-assume-omega-linear-in-E">
<span class="eqno">(3)</span>\[\mathrm{log}(\Omega_B(E_B)) \approx \alpha + \beta E_B\]</div>
</div>
<p>Any function can be approximated as linear over sufficiently small intervals (aside from pathological cases like
singularities and discontinuities).  So essentially we are assuming that we only care about a small range of values for
<span class="math">\(E_B\)</span>.  In practice, what this really means is that B must be much larger than A.  However much the energy of A
fluctuates, it must only have a very small effect on B.  A is a small test tube while B is a large water bath.  A is a
cup of coffee while B is the surrounding room.</p>
<p>We now want to answer the following question: if the total energy of the system is <span class="math">\(E_T\)</span>, what is the probability
of A being in <em>one particular microstate</em> whose energy is <span class="math">\(E_A\)</span>?</p>
<p>We can reason this out in a series of steps, using each of our assumptions in turn.</p>
<ol class="arabic">
<li><p class="first">The complete system can be in <span class="math">\(\Omega_T(E_T)\)</span> possible microstates.  By the PoEapP, every one of them is
equally probable.  Some of those microstates involve A being in the desired microstate and others do not.  We
therefore conclude:</p>
<p><em>The probability of A being in the desired microstate equals the fraction of microstates of the whole system for
which A is in that microstate.</em></p>
</li>
<li><p class="first">By the first assumption above, the energy of B is <span class="math">\(E_B = E_T-E_A\)</span>.</p>
</li>
<li><p class="first">By the second assumption, <span class="math">\(\Omega_B\)</span> is completely independent of what microstate A is in, and depends only on
<span class="math">\(E_B\)</span>.  Therefore, the number of microstates of the whole system for which A is in the desired microstate is
simply equal to <span class="math">\(\Omega_B(E_B) = \Omega_B(E_T-E_A)\)</span>.</p>
</li>
<li><p class="first">By the third assumption, <span class="math">\(\Omega_B(E_B) = e^{\alpha+\beta E_B}\)</span>.</p>
</li>
</ol>
<p>Combining these results, we find the probability of A being in the desired microstate is</p>
<div class="math">
\[p(E_A) \propto \Omega_B(E_T-E_A) \propto e^{-\beta E_A}\]</div>
<p>It is conventional to write this in a slightly different form by defining</p>
<div class="math" id="equation-define-temperature">
<span class="eqno">(4)</span>\[\frac{1}{kT} \equiv \beta \equiv \frac{\partial \mathrm{log}(\Omega_B(E))}{\partial E}\]</div>
<p><span class="math">\(T\)</span> is known as the <em>temperature</em> and <span class="math">\(k\)</span> is <em>Boltzmann&#8217;s constant</em>, which equals 1.3806488·10<sup>-23</sup>
Joules/Kelvin.  <span class="math">\(\beta\)</span> is called the <em>inverse temperature</em>.  With this definition, the probability can be
written as</p>
<div class="math">
\[p(E_A) \propto e^{-E_A/kT}\]</div>
<p>This last step is quite a strange one.  For no obvious reason, we have just introduced a constant with a seemingly
arbitrary value, along with a completely new set of units.  (Just what are &#8220;Kelvins&#8221;?)  Of course, you probably have
already figured out why.  The concept of &#8220;temperature&#8221; was established long before statistical mechanics was developed,
and this definition is needed to make the statistical definition match the pre-existing one.  We will examine the
correspondence in Chapter <a class="reference internal" href="interpretation.html#interpretation-of-statistical-quantities">4</a>.  For the moment, though, just think of it as
an arbitrary mathematical definition.</p>
<p>We have almost answered our question.  The only thing still missing is the proportionality constant.  That is easily
found: we just require that the probabilities of all microstates add to 1.  (The system is certain to be in <em>some</em>
state, after all.)  The normalization constant is therefore given by</p>
<div class="math" id="equation-define-partition-function">
<span class="eqno">(5)</span>\[Z = \sum e^{-E_A/kT}\]</div>
<p>where the sum is taken over all microstates of A.  <span class="math">\(Z\)</span> is called the <em>partition function</em>.  Aside from being a
normalization constant, it turns out to be an interesting function in its own right with some useful properties.  We
will see more of it later.</p>
<p>We can now give the probability for A to be in the desired microstate:</p>
<div class="math" id="equation-maxwell-boltzmann">
<span class="eqno">(6)</span>\[p(E_A) = \frac{e^{-E_A/kT}}{Z}\]</div>
<p>This is called the <em>Maxwell-Boltzmann distribution</em>, and it is probably the single most important equation in this
chapter (or possibly even in this entire book).  Maxwell originally derived it in 1860 based on a mechanical model of
gas molecules.  In the following years it was repeatedly re-derived based on a variety of arguments that extended its
generality.  As you have seen, it is not in any way specific to gas molecules or classical mechanics.  It is valid for
any system that satisfies a very general set of assumptions.</p>
</div>
<div class="section" id="thermodynamic-forces">
<span id="thermodynaic-forces"></span><h2>2.6. Thermodynamic Forces<a class="headerlink" href="#thermodynamic-forces" title="Permalink to this headline">¶</a></h2>
<p>In the last section we assumed the only macroscopic variable we cared about was energy.  Let us now extend this to more
general cases.  As a concrete example, assume we have two macroscopic variables: energy and volume.  Perhaps we are
dealing with a balloon filled with helium, so it can stretch and contract, exchanging volume as well as energy with the
surrounding air.  The density of states is now a function of both variables, <span class="math">\(\Omega(E, V)\)</span>.</p>
<p>We can repeat the exact same argument as in the last section, simply replacing <span class="math">\(E\)</span> with <span class="math">\(V\)</span>.  This time we
assume <span class="math">\(\mathrm{log}(\Omega_B)\)</span> is linear in both variables:</p>
<div class="math" id="equation-assume-omega-linear-in-V">
<span class="eqno">(7)</span>\[\mathrm{log}(\Omega_B(E_B, V_B)) \approx \alpha + \beta E_B + \gamma V_B\]</div>
<p>from which we conclude that the probability of a microstate is given by</p>
<div class="math">
\[p(E_A, V_A) \propto \Omega_B(E_T-E_A, V_T-V_A) \propto e^{-\beta E_A-\gamma V_A}\]</div>
<p>Once again it is conventional to write this in a slightly different form by defining a new quantity:</p>
<div class="math" id="equation-define-pressure">
<span class="eqno">(8)</span>\[P \equiv kT \frac{\partial \mathrm{log}(\Omega_B(E, V))}{\partial V}\]</div>
<p><span class="math">\(P\)</span> is called the <em>pressure</em>.  Using this definition, the probability for A to be in a particular microstate is</p>
<div class="math" id="equation-maxwell-boltzmann-with-PV">
<span class="eqno">(9)</span>\[p(E_A, V_A) = \frac{e^{-(E_A+PV_A)/kT}}{Z}\]</div>
<p>where the partition function now equals</p>
<div class="math" id="equation-enthalpy-partition-function">
<span class="eqno">(10)</span>\[Z = \sum e^{-(E_A+PV_A)/kT}\]</div>
<p>There is nothing special about volume.  The same calculation can be done for any macroscopic variable, producing an
identical result.  One other example that is especially important in thermodynamics is <span class="math">\(N\)</span>, the number of
particles in the system.  Perhaps we are studying a box filled with gas, but the box has a small hole in it allowing
molecules to diffuse in and out.  Rather than defining subsystem A to be particular set of molecules (whatever volume
of space they occupy), we instead define it to be a particular volume of space (whatever molecules it happens to contain
at any moment).  We then define</p>
<div class="math" id="equation-define-chemical-potential">
<span class="eqno">(11)</span>\[\mu \equiv -kT \frac{\partial \mathrm{log}(\Omega_B(E, N))}{\partial N}\]</div>
<p><span class="math">\(\mu\)</span> is called the <em>chemical potential</em>.  The negative sign in front of it is just a matter of convention.  The
probability of a microstate is</p>
<div class="math" id="equation-maxwell-boltzmann-with-mu-N">
<span class="eqno">(12)</span>\[p(E_A, N_A) = \frac{e^{-(E_A-\mu N_A)/kT}}{Z}\]</div>
<p>Quantities like <span class="math">\(P\)</span> and <span class="math">\(\mu\)</span> are called <em>thermodynamic forces</em>.  Each one is said to be <em>conjugate</em> to the
macroscopic variable we differentiated with respect to.  Together, the macroscopic variable and the thermodynamic force
(<span class="math">\(V\)</span> and <span class="math">\(P\)</span>, or <span class="math">\(N\)</span> and <span class="math">\(\mu\)</span>) form a <em>conjugate pair</em>.</p>
<p>&#8220;Thermodynamic force&#8221; is another very suggestive name.  How do they relate to forces of the more conventional sort?  Do
they act to produce accelerations?  Are they derivatives of potential functions?  We will examine these questions in
Chapter <a class="reference internal" href="interpretation.html#interpretation-of-statistical-quantities">4</a>.  As with everything else in this chapter, just treat them as
arbitrary mathematical definitions for now.</p>
<p>Having said that, I now need to indulge in a brief rant.  <span class="math">\(\mu\)</span> is a &#8220;thermodynamic force&#8221;, but it is also called
the &#8220;chemical potential&#8221;.  So is it a force, or is it a potential?  They are not the same thing!  Just to make matters
worse, we will soon encounter another type of quantity called a &#8220;thermodynamic potential&#8221; (of which <span class="math">\(\mu\)</span> is <em>not</em>
an example).  Could we at least use consistent terminology?  Sadly, the answer is no, we cannot.  These names were
established long ago, and now it is impossible to change them, even when they clearly do not make sense.</p>
</div>
<div class="section" id="probabilities-of-macrostates">
<h2>2.7. Probabilities of Macrostates<a class="headerlink" href="#probabilities-of-macrostates" title="Permalink to this headline">¶</a></h2>
<p>Now that we know how to calculate the probability of the system being in a microstate, we can easily do the same for a
macrostate.  Just add up the probabilities for all the microstates it contains.  For simplicity, assume the only
macroscopic variable of interest is energy.  The probability of a macrostate is</p>
<div class="math" id="equation-macrostate-probability">
<span class="eqno">(13)</span>\[p(E_A) = \frac{1}{Z} \sum e^{-E_A/kT}\]</div>
<p>The sum is taken over every microstate contained in the macrostate.  If there are other macroscopic variables, just use
the appropriate exponential factor.  For example, if the macrostate is defined by both energy and volume, replace
<span class="math">\(E_A\)</span> by <span class="math">\(E_A+PV_A\)</span>.</p>
<p>Every term of the sum has exactly the same value, so instead of summing we can just multiply by the number of
microstates:</p>
<div class="math" id="equation-macrostate-probability-2">
<span class="eqno">(14)</span>\[p(E_A) = \Omega_A(E_A) \frac{e^{-E_A/kT}}{Z}\]</div>
<p>We now define another new quantity:</p>
<div class="math" id="equation-define-entropy">
<span class="eqno">(15)</span>\[S = k \cdot \mathrm{log}(\Omega_A)\]</div>
<p><span class="math">\(S\)</span> is called the <em>entropy</em> of the macrostate.  It is just another way of measuring the number of microstates
that make it up.  Given this definition, we can rewrite the probability as</p>
<div class="math" id="equation-maxwell-boltzmann-with-TS">
<span class="eqno">(16)</span>\[p(E_A) = \frac{e^{-(E_A-TS)/kT}}{Z}\]</div>
</div>
<div class="section" id="thermodynamic-potentials">
<span id="thermodynaic-potentials"></span><h2>2.8. Thermodynamic Potentials<a class="headerlink" href="#thermodynamic-potentials" title="Permalink to this headline">¶</a></h2>
<p>We now know how to compute the probability of finding a system in lots of different kinds of states: microstates or
macrostates, specified by arbitrary sets of macroscopic variables.  In every case, the probability takes exactly the
same form:</p>
<div class="math" id="equation-maxwell-boltzmann-with-phi">
<span class="eqno">(17)</span>\[p = \frac{e^{-\Phi/kT}}{Z}\]</div>
<p>where the only difference is the quantity <span class="math">\(\Phi\)</span> appearing in the exponent.  This suggests the idea of
<em>thermodynamic potentials</em>, energy-like functions that capture the differences between different probability
distributions.  Several of the most common thermodynamic potentials have special names:</p>
<div class="math" id="equation-thermodynamic-potentials">
<span class="eqno">(18)</span>\[\begin{split}\begin{array}{rcll}
H &amp;=&amp; E+PV &amp; \text{(Enthalpy)} \\
A &amp;=&amp; E-TS &amp; \text{(Helmholtz free energy)} \\
G &amp;=&amp; E+PV-TS &amp; \text{(Gibbs free energy)} \\
\Phi_G &amp;=&amp; E-\mu N-TS &amp; \text{(Grand potential)}
\end{array}\end{split}\]</div>
<p>The term <em>free energy</em> can also be used more generally to refer to any thermodynamic potential that describes the
probabilities of macrostates (that is, any thermodynamic potential that includes a <span class="math">\(TS\)</span> term).  The Helmholtz
free energy, the Gibbs free energy, and the grand potential are all examples of free energies.</p>
<p>You can think about the differences between thermodynamic potentials in two equivalent ways.  First, you can think of
starting with <span class="math">\(E\)</span> and then adding in terms based on the ensemble you want to use.  You want volume to be
variable?  Then add <span class="math">\(PV\)</span>.  You want to work with macrostates instead of microstates?  Subtract <span class="math">\(TS\)</span>.  And
so on.</p>
<p>Alternatively, you can think of all these potentials as special cases of a single potential that includes all possible
terms.  If the volume is held fixed, then <span class="math">\(PV\)</span> is a constant and can be ignored.  It just changes the
proportionality constant, which gets normalized away when we require the probabilities to add to 1.  If the number of
particles is fixed, then <span class="math">\(\mu N\)</span> is similarly a constant and can be ignored.  If you are working with microstates
then <span class="math">\(TS\)</span> is zero.  (Think of a microstate as being a tiny macrostate with exactly one microstate, so
<span class="math">\(\Omega\)</span> is 1 and <span class="math">\(S\)</span> is 0).</p>
<p>Just as there are special names for common thermodynamic potentials, some of the corresponding ensembles also have
special names.  These names are purely historical.  They do not have any particular meaning, but they are still widely
used, so you will need to know them.</p>
<p>The <em>microcanonical ensemble</em> refers to an isolated system whose energy is constant.  It has equal probability of being
in any microstate with the specified energy, and of course no chance at all of being in any microstate with a different
energy.</p>
<p>The <em>canonical ensemble</em> refers to a system that can exchange energy with a heat bath at a specified temperature.  The
corresponding thermodynamic potential is <span class="math">\(E\)</span> for microstates, or <span class="math">\(E-TS\)</span> for macrostates.</p>
<p>The <em>grand canonical ensemble</em> refers to a system that can exchange both energy and particles with a heat bath of
specified temperature and chemical potential.  The corresponding thermodynamic potential is <span class="math">\(E-\mu N\)</span> for
microstates, or <span class="math">\(E-\mu N-TS\)</span> for macrostates.</p>
</div>
<div class="section" id="averages">
<span id="id3"></span><h2>2.9. Averages<a class="headerlink" href="#averages" title="Permalink to this headline">¶</a></h2>
<p>The average (or <em>mean</em>) of a quantity <span class="math">\(x\)</span> is defined as</p>
<div class="math" id="equation-define-average">
<span class="eqno">(19)</span>\[\langle x \rangle \equiv \sum_i x_i p_i\]</div>
<p>where <span class="math">\(x_i\)</span> is its value in the i&#8217;th state, and <span class="math">\(p_i\)</span> is the probability of that state.  The sum can be over
either microstates or (if <span class="math">\(x\)</span> is a macroscopic variable) macrostates.  This is an ensemble average.  Its value
depends on the probabilities of the system being in different states; or to say that another way, it depends on what
ensemble we are calculating the average for.  If <span class="math">\(\Phi\)</span> is the thermodynamic potential for the ensemble, it equals</p>
<div class="math" id="equation-ensemble-average">
<span class="eqno">(20)</span>\[\langle x \rangle = \frac{\sum_i x_i e^{-\Phi_i/kT}}{\sum_i e^{-\Phi_i/kT}}\]</div>
<p>Two important identities follow directly from the above definition:</p>
<div class="math" id="equation-average-of-sum">
<span class="eqno">(21)</span>\[\langle x+y \rangle = \sum_i (x_i+y_i) p_i = \sum_i x_i p_i + \sum_i y_i p_i = \langle x \rangle + \langle y \rangle\]</div>
<p>and, if <span class="math">\(C\)</span> is a constant,</p>
<div class="math" id="equation-average-times-constant">
<span class="eqno">(22)</span>\[\langle Cx \rangle = \sum_i Cx_i p_i = C \sum_i x_i p_i = C \langle x \rangle\]</div>
<p>Just because <span class="math">\(x\)</span> has a particular average value, that does not mean it is always exactly equal to that.  Sometimes
it is more and sometimes it is less.  It can be very useful to know how much a quantity tends to vary about its average.
Does it stay within a narrow range, or does it vary widely?  A useful measure of this is its <em>variance</em>, defined as</p>
<div class="math" id="equation-define-variance">
<span class="eqno">(23)</span>\[Var(x) \equiv \langle \left(x-\langle x \rangle \right)^2 \rangle\]</div>
<p>Remembering that <span class="math">\(\langle x \rangle\)</span> is a constant, we can derive a useful identity for the variance:</p>
<div class="math" id="equation-variance-alternate-form">
<span class="eqno">(24)</span>\[\begin{split}Var(x) &amp;= \langle x^2 - 2x \langle x \rangle + \langle x \rangle ^2 \rangle \\
&amp;= \langle x^2 \rangle - 2 \langle x \rangle \langle x \rangle + \langle x \rangle^2 \\
&amp;= \langle x^2 \rangle - \langle x \rangle^2\end{split}\]</div>
<p>Another common measure of how much a value tends to vary is its <em>standard deviation</em>, which is simply the square root
of the variance.  It is represented by the symbol <span class="math">\(\sigma\)</span>.  A good rule of thumb is that about 2/3 of the time,
the value will be between <span class="math">\(\langle x \rangle-\sigma\)</span> and <span class="math">\(\langle x \rangle+\sigma\)</span>.  The exact fraction
depends on the probability distribution, of course.  We will examine this further in the next chapter.</p>
<p>I mentioned before that the partition function has some interesting and useful properties.  One of them is that
derivatives of <span class="math">\(\mathrm{log}(Z)\)</span> tend to give averages.  Remember that the partition function is defined as</p>
<div class="math">
\[Z = \sum e^{-\beta \Phi} = \sum e^{-\Phi/kT}\]</div>
<p>For example,</p>
<div class="math" id="equation-derive-logZ-beta">
<span class="eqno">(25)</span>\[\begin{split}-\frac{\partial \mathrm{log}(Z)}{\partial \beta} &amp;= -\frac{1}{Z} \frac{\partial Z}{\partial \beta} \\
&amp;= -\frac{1}{Z} \sum \frac{\partial e^{-\beta \Phi}}{\partial \beta} \\
&amp;= \frac{1}{Z} \sum \Phi e^{-\beta \Phi} \\
&amp;= \langle \Phi \rangle\end{split}\]</div>
<p>Another useful case is to take the derivative with respect to a state variable (either a microscopic or a macroscopic
one).</p>
<div class="math" id="equation-derive-logZ-state-variable">
<span class="eqno">(26)</span>\[\begin{split}-kT \frac{\partial \mathrm{log}(Z)}{\partial x} &amp;= -\frac{kT}{Z} \frac{\partial Z}{\partial x} \\
&amp;= -\frac{kT}{Z} \sum \frac{\partial e^{-\Phi/kT}}{\partial x} \\
&amp;= \frac{1}{Z} \sum \frac{\partial \Phi}{\partial x} e^{-\Phi/kT} \\
&amp;= \left\langle \frac{\partial \Phi}{\partial x} \right\rangle\end{split}\]</div>
</div>
<div class="section" id="quantum-statistical-mechanics">
<h2>2.10. Quantum Statistical Mechanics<a class="headerlink" href="#quantum-statistical-mechanics" title="Permalink to this headline">¶</a></h2>
<p>For simplicity, I will mostly rely on classical mechanics in this book.  But nearly everything I say applies equally
well to quantum mechanics.</p>
<p>For a quantum system, a microstate simply means a value of the wave function (or, if we need to discretize a continuum
of states, a tiny volume of Hilbert space).  Of course you can describe the wave function using any set of basis
functions you want.  The &#8220;microscopic variables&#8221; of the system are just the amplitudes of the basis functions.  If they
happen to be position eigenstates, then the microscopic variables are the values of the wave function at each point in
space.  But you can just as easily use momentum eigenstates, energy eigenstates, or any other basis you choose.</p>
<p>A &#8220;macroscopic variable&#8221;, on the other hand, is defined as the expectation value of an operator.  If <span class="math">\(Y\)</span> is the
operator corresponding to some measurable quantity <span class="math">\(y\)</span> and the system is in microstate
<span class="math">\(\left| \Psi \right\rangle\)</span>, then</p>
<div class="math" id="equation-quantum-macroscopic-variable">
<span class="eqno">(27)</span>\[y \equiv \left\langle \Psi \right| Y \left| \Psi \right\rangle\]</div>
<p>This always has a well defined value, even if <span class="math">\(\left| \Psi \right\rangle\)</span> is not an eigenstate of the operator
<span class="math">\(Y\)</span>.  A particularly important case is energy, which is the expectation value of the Hamiltonian:</p>
<div class="math" id="equation-quantum-energy">
<span class="eqno">(28)</span>\[E \equiv \left\langle \Psi \right| H \left| \Psi \right\rangle\]</div>
<p>As long as the system remains isolated, its energy is constant.  Conservation of energy applies just as well to quantum
mechanics as to classical mechanics.</p>
<p>When dealing with quantum systems, we need to be careful to distinguish between different types of probability.  In
statistical mechanics, probabilities always refer to either ensemble averages or time averages.  The &#8220;probability&#8221; of a
variable having a particular value refers to either a fraction of the members of an ensemble, or to a fraction of time.
But quantum mechanics also has its own probabilities that apply even when a system is in a single known state.  They
describe the probability that a measurement will produce a certain result, <em>given</em> that the system is in a particular
state.</p>
<p>The probabilistic features of quantum mechanics <em>only</em> come up when you perform a measurement, which is to say, when you
let the system interact with an external measuring device.  As long as the system stays isolated, quantum mechanics is
fully deterministic.  The system is always in a well defined state, and every microscopic and macroscopic variable has a
single well defined value at every moment in time.</p>
<p>It is even possible that the probabilistic features of quantum mechanics are <em>also</em> statistical in nature.  When you
allow a system to interact with an external measuring device, that will necessarily introduce noise into the system.
The state of the system is no longer definitely known, because it is subject to unknown forces.  Is it surprising, then,
that we cannot predict the result with complete certainty?</p>
<p>This is a large subject of its own, and a very controversial one.  It can be proven that <em>if</em> quantum mechanics is
statistical in nature, it must necessarily possess one or more unintuitive properties such as nonlocality or
retrocausality.  But quantum mechanics is already one of the most unintuitive physical theories ever developed, so that
is hardly an argument one way or the other!  Some physicists would even argue that we already have good evidence for
both nonlocality <em>and</em> retrocausality coming from completely unrelated directions.  If so, then statistical
interpretations of quantum mechanics might well be among the very simplest and most intuitive ones.</p>
<p>In any case, when applying statistical mechanics to quantum systems, be sure to distinguish the &#8220;probabilities&#8221; due to
statistical ensembles from the &#8220;probabilities&#8221; due to quantum mechanics itself.  At any time other than when you are
actually in the middle of making a measurement, the former ones are the only kind that apply.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">2. The Statistical Description of Physical Systems</a><ul>
<li><a class="reference internal" href="#microstates-and-macrostates">2.1. Microstates and Macrostates</a></li>
<li><a class="reference internal" href="#the-density-of-states">2.2. The Density of States</a></li>
<li><a class="reference internal" href="#the-postulate-of-equal-a-priori-probabilities">2.3. The Postulate of Equal <em>a priori</em> Probabilities</a></li>
<li><a class="reference internal" href="#time-averages-and-ensemble-averages">2.4. Time Averages and Ensemble Averages</a></li>
<li><a class="reference internal" href="#the-maxwell-boltzmann-distribution">2.5. The Maxwell-Boltzmann Distribution</a></li>
<li><a class="reference internal" href="#thermodynamic-forces">2.6. Thermodynamic Forces</a></li>
<li><a class="reference internal" href="#probabilities-of-macrostates">2.7. Probabilities of Macrostates</a></li>
<li><a class="reference internal" href="#thermodynamic-potentials">2.8. Thermodynamic Potentials</a></li>
<li><a class="reference internal" href="#averages">2.9. Averages</a></li>
<li><a class="reference internal" href="#quantum-statistical-mechanics">2.10. Quantum Statistical Mechanics</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="introduction.html"
                        title="previous chapter">1. Introduction</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="largenumbers.html"
                        title="next chapter">3. Mathematical Interlude: Very Large Numbers</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="largenumbers.html" title="3. Mathematical Interlude: Very Large Numbers"
             >next</a> |</li>
        <li class="right" >
          <a href="introduction.html" title="1. Introduction"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Introduction to Statistical Mechanics</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2014-2015, Peter Eastman.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>